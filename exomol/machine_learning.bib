%%% General Machine Learning references file %%%

% This file contains references for general machine learning methods and algorithms.
% References to specific applications are contained within ML.bib

% Ordered by year of publication, then method name.

% Key convention:
% YYAABBCC_XXXX.ML
% - YY = year
% - AA = first 2 letters of first author's surname
% - BB = first 2 letters of second author's surname (if applicable)
% - CC = first 2 letters of third author's surname (if applicable)
% - XX = short form of method name

@article{23HeGi_GELU.ML,
  title={Gaussian Error Linear Units (GELUs)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2023},
  eprint={1606.08415},
  archivePrefix={arXiv},
  url={https://arxiv.org/abs/1606.08415}
}

@article{19Ag_ReLU.ML,
  title={Deep Learning using Rectified Linear Units (ReLU)}, 
  author={Agarap, Abien Fred},
  journal={arXiv preprint arXiv:1803.08375},
  year={2019},
  eprint={1803.08375},
  archivePrefix={arXiv},
  url={https://arxiv.org/abs/1803.08375}
}

@inproceedings{19PaGrMa_Pytorch.ML,
  title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K\"{o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  booktitle = {Advances in Neural Information Processing Systems 32},
  pages = {8024--8035},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.}
}

@inproceedings{17KiBa_Adam.ML,
  author={Kingma, Diederik P. and Ba, Jimmy},
  booktitle={3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  title={Adam: A Method for Stochastic Optimization},
  year={2015},
  url={http://arxiv.org/abs/1412.6980},
  abstract={We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The hyper-parameters have intuitive interpretations and typically require little tuning. Adam converges fast and outperforms other stochastic optimization methods in practice.}
}

@inproceedings{16GaGh_MCDrop.ML,
  title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  pages = {1050--1059},
  year = {2016},
  editor = {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = {48},
  series = {Proceedings of Machine Learning Research},
  address = {New York, New York, USA},
  publisher = {PMLR},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.}
}

@article{16BaKiHi_LayerNorm.ML,
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  title={Layer Normalization},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016},
  abstract={Training very deep neural networks is a challenging problem. We propose a new normalization technique called layer normalization. Like batch normalization, layer normalization performs the same operation as whitening the inputs to a layer. However, unlike batch normalization, layer normalization does not depend on mini-batch statistics. This makes it more suitable for recurrent neural networks and online learning, where batch sizes may be small or variable. Layer normalization normalizes all of the summed inputs to a neuron on a single training case, rather than across a mini-batch. The method is simple to implement and computationally efficient. We show that layer normalization can significantly accelerate the training of deep neural networks compared to other normalization methods.},
  doi={10.48550/arXiv.1607.06450}
}

@article{10AbWi_PCA.ML,
  author={Abdi, Hervé and Williams, Lynne J.},
  title={Principal component analysis},
  journal={Wiley Interdisciplinary Reviews: Computational Statistics},
  year={2010},
  volume={2},
  pages={433--459},
  abstract={Principal component analysis (PCA) is a multivariate data analysis technique that has been widely used in the sciences for more than a century. PCA is often used as an exploratory tool to analyze data tables in which observations are described by several inter-correlated quantitative dependent variables. The goal of PCA is to extract the important information from the data table and to represent it as a set of new orthogonal variables called principal components (PCs). These PCs are ordered so that the first few retain most of the variation present in all of the original variables. This article presents an overview of the basic concepts of PCA, its mathematical derivation, and some examples of its use.},
  doi={10.1002/wics.101}
}

@article{06GeErWe_ET.ML,
  author={Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},
  title={Extremely randomized trees},
  journal={Machine Learning},
  year={2006},
  volume={63},
  pages={3--42},
  abstract={This paper proposes a new tree-based ensemble method for supervised classification and regression problems. It essentially consists of randomizing strongly both attribute and cut-point choice while splitting a tree node. In the extreme case, it builds totally randomized trees whose structures are independent of the output values of the learning sample. The strength of the randomization can be tuned to problem specifics by the appropriate choice of a parameter. We evaluate the robustness of the default choice of this parameter, and we also provide insight on how to adjust it in particular situations. Besides accuracy, the main strength of the resulting algorithm is computational efficiency. A bias/variance analysis of the Extra-Trees algorithm is also provided as well as a geometrical and a kernel characterization of the models induced.},
  doi={10.1007/s10994-006-6226-1}
}

@article{01Fr_GB.ML,
  author = {Friedman, Jerome H.},
  title = {Greedy function approximation: A gradient boosting machine},
  volume = {29},
  journal = {The Annals of Statistics},
  pages = {1189--1232},
  abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent “boosting” paradigm is developed for additive expansions based on any fitting criterion.Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such “TreeBoost” models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
  keywords = {boosting, decision trees, Function estimation, robust nonparametric regression},
  year = {2001},
  doi = {10.1214/aos/1013203451}
}

@article{01Br_RF.ML,
  author={Breiman, Leo},
  title={Random Forests},
  journal={Machine Learning},
  year={2001},
  volume={45},
  pages={5--32},
  abstract={Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International Conference on Machine Learning, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  issn={1573-0565},
  doi={10.1023/A:1010933404324}
}

@article{97FrSc_ADA.ML,
  title = {A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting},
  author = {Freund, Yoav and Schapire, Robert E},
  journal = {Journal of Computer and System Sciences},
  volume = {55},
  pages = {119--139},
  year = {1997},
  doi = {10.1006/jcss.1997.1504},
  abstract = {In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone–Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in $\mathbb{R}^n$. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.}
}

@article{92Wo_Stack.ML,
  title = {Stacked generalization},
  journal = {Neural Networks},
  volume = {5},
  pages = {241--259},
  year = {1992},
  doi = {10.1016/S0893-6080(05)80023-1},
  author = {Wolpert, David H.},
  keywords = {Generalization and induction, Combining generalizers, Learning set preprocessing, cross-validation, Error estimation and correction},
  abstract = {This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.}
}

@article{89Hi_MLP.ML,
  title = {Connectionist learning procedures},
  journal = {Artificial Intelligence},
  volume = {40},
  pages = {185--234},
  year = {1989},
  doi = {10.1016/0004-3702(89)90049-0},
  author = {Hinton, Geoffrey E.},
  abstract = {A major goal of research on networks of neuron-like processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units which are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative, with respect to the connection strength, of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple, gradient-descent learning procedures work well for small tasks and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can be applied to larger, more realistic tasks.}
}

@article{67CoHa_KNN.ML,
  author={Cover, T. and Hart, P.},
  journal={IEEE Transactions on Information Theory}, 
  title={Nearest neighbor pattern classification}, 
  year={1967},
  volume={13},
  pages={21--27},
  abstract={The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of errorRof such a rule must be at least as great as the Bayes probability of errorR^{\ast}--the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in theM-category case thatR^{\ast} \leq R \leq R^{\ast}(2 --MR^{\ast}/(M-1)), where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
  doi={10.1109/TIT.1967.1053964}
}

@article{64Hu_HuberLoss.ML,
  author = {Huber, P.J.},
  journal = {The Annals of Mathematical Statistics},
  title = {Robust Estimation of a Location Parameter},
  volume = {35},
  year = {1964},
  pages={73--101},
  doi={10.1214/aoms/1177703732}
}